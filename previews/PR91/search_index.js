var documenterSearchIndex = {"docs":
[{"location":"uncertainty_predictions.html#Computing-Pose-Uncertainty","page":"Computing Pose Uncertainty","title":"Computing Pose Uncertainty","text":"","category":"section"},{"location":"uncertainty_predictions.html","page":"Computing Pose Uncertainty","title":"Computing Pose Uncertainty","text":"TODO: Write this section.","category":"page"},{"location":"C_interface.html#C-Interface","page":"C Interface","title":"C Interface","text":"","category":"section"},{"location":"C_interface.html","page":"C Interface","title":"C Interface","text":"Beyond the Julia and Python interfaces, we also expose core parts of our library as a ahead-of-time compiled C library using JuliaC.jl. This is currently in a proof-of-concept state since not all public functions are exported, but it will not be hard to expand this once the need is there.","category":"page"},{"location":"C_interface.html#Usage-example","page":"C Interface","title":"Usage example","text":"","category":"section"},{"location":"C_interface.html","page":"C Interface","title":"C Interface","text":"A usage example can be found at  main.c which we reprint here:","category":"page"},{"location":"C_interface.html","page":"C Interface","title":"C Interface","text":"import Markdown\nMarkdown.parse(\"\"\"\n```C\n$(readchomp(joinpath(\"..\", \"..\", \"juliac\", \"main.c\")))\n```\n\"\"\")","category":"page"},{"location":"C_interface.html","page":"C Interface","title":"C Interface","text":"Next to the main.c file one can find the Makefile which outlines how to compile using the generated C library.","category":"page"},{"location":"C_interface.html","page":"C Interface","title":"C Interface","text":"warning: Warning\nAt this time, only estimate_pose_6dof is properly supported, although there's nothing stopping us from supporting the rest.","category":"page"},{"location":"noise_models.html#Noise-Models","page":"Noise Models","title":"Noise Models","text":"","category":"section"},{"location":"noise_models.html","page":"Noise Models","title":"Noise Models","text":"See ","category":"page"},{"location":"noise_models.html","page":"Noise Models","title":"Noise Models","text":"UncorrGaussianNoiseModel,\nUncorrProductNoiseModel,\nCorrGaussianNoiseModel, and\nNoiseModel.","category":"page"},{"location":"python_interface_template.html#Python-Interface","page":"Python Interface","title":"Python Interface","text":"","category":"section"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"Several attempts have been made to make this module easily callable from python. At this time, we recommend using the python package JuliaCall. Simply install juliacall alongside numpy using your favorite method, for instance","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"uv add juliacall\nuv add numpy","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"and use juliacall to install RunwayLib and the PythonCall extension which enables the python API.","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"from juliacall import Main as jl\njl.Pkg.add(url=\"http://github.com/RomeoV/RunwayLib.jl\")  # will be registered soon","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"You only need to do this once. Now you should be able to use RunwayLib from python like so:","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"{{PYTHON_EXAMPLE}}","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"Notice that we can also directly wrap np.array for WorldPoint and the other, e.g.:","category":"page"},{"location":"python_interface_template.html","page":"Python Interface","title":"Python Interface","text":"points2d_np = [\n    np.array([2284.8, 619.32]),\n    np.array([2355.2, 620.90]),\n    np.array([2399.5, 903.30]),\n    np.array([2252.4, 904.09]),\n]\npoint2d = [jl.ProjectionPoint(p) for p in points2d_np]","category":"page"},{"location":"integrity_check.html#Integrity-Check","page":"Integrity Check","title":"Integrity Check","text":"","category":"section"},{"location":"integrity_check.html","page":"Integrity Check","title":"Integrity Check","text":"In our paper [1] we introduce an \"integrity check\" that let's us compare the reprojection error of the estimated pose to the magnitude of the predicted uncertainties by adoping an algorithm similar to RAIM [2]. This figure provides a brief illustration, with more details in [1].","category":"page"},{"location":"integrity_check.html","page":"Integrity Check","title":"Integrity Check","text":"(Image: An illustration of the integrity check.)","category":"page"},{"location":"integrity_check.html#RunwayLib.compute_integrity_statistic-integrity_check","page":"Integrity Check","title":"RunwayLib.compute_integrity_statistic","text":"compute_integrity_statistic(\n    cam_pos::WorldPoint, cam_rot::RotZYX,\n    world_pts::AbstractVector{<:WorldPoint},\n    observed_pts::AbstractVector{<:ProjectionPoint},\n    noise_cov::Union{<:AbstractMatrix, <:NoiseModel},\n    camconfig=CAMERA_CONFIG_OFFSET\n)\n\nRun the integrity check described in [1]. We can use this for runtime assurance to judge whether the measurements and uncertainties are consistent with the parameters of the problem.\n\nReturns\n\nNamedTuple containing\n\nstat: The RAIM-adaptation statistic;\np_value: p-value of Null-hypothesis. If this drops below, say, 5% then we can \"reject\", i.e., have a failure;\ndofs: degrees of freedom (for Î§Â² distribution); and\nsome other information.\n\nSee also\n\nWorldPoint, RotZYX, ProjectionPoint, NoiseModel.\n\n\n\n\n\n","category":"function"},{"location":"integrity_check.html","page":"Integrity Check","title":"Integrity Check","text":"R.Â Valentin, S.Â M.Â Katz, A.Â B.Â Carneiro, D.Â Walker and M.Â J.Â Kochenderfer. Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System, arXivÂ preprintÂ arXiv:2508.09732 (2025).\n\n\n\nM.Â Joerger, F.-C.Â Chan and B.Â Pervan. Solution separation versus residual-based RAIM. NAVIGATION:Â JournalÂ ofÂ theÂ InstituteÂ ofÂ Navigation 61, 273â€“291 (2014).\n\n\n\n","category":"page"},{"location":"test_container.html#Experiment-1","page":"Experiment 1","title":"Experiment 1","text":"","category":"section"},{"location":"test_container.html","page":"Experiment 1","title":"Experiment 1","text":"using RunwayLib\nusing Bonito, BonitoBook\nApp() do\n    path = normpath(joinpath(dirname(pathof(RunwayLib)), \"..\", \"docs\", \"src\", \"test.md\"))\n    BonitoBook.InlineBook(path)\nend","category":"page"},{"location":"benchmarks.html#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"<style>\n@media screen and (min-width: 1056px) {\n  #documenter .docs-main {\n    max-width: none !important;\n  }\n  #documenter .docs-main pre {\n    max-width: 52rem !important;\n  }\n  #documenter .docs-main p,\n  #documenter .docs-main ul,\n  #documenter .docs-main ol,\n  #documenter .docs-main h1,\n  #documenter .docs-main h2,\n  #documenter .docs-main h3,\n  #documenter .docs-main h4,\n  #documenter .docs-main h5,\n  #documenter .docs-main h6 {\n    max-width: 52rem !important;\n  }\n}\n</style>","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"Here we present a benchmarking example of the simple estimation presented in Getting Started. In particular, this is an example of how we can make use of statically sized arrays via StaticArrays.jl for wide parts of the code and optimization, letting us eliminate most allocations. Below in Profiling the Callstack we will go into further details. Here, we further make use of pre-allocated caches for various steps of the optimization such as the Jacobians, and algorithmic magic such as a well-tuned implementation of LevenbergMarquardt with GeodesicAcceleration.","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"using RunwayLib, Unitful.DefaultSymbols, Rotations, StaticArrays\nusing BenchmarkTools\nrunway_corners = SA[\n    WorldPoint(0.0m, 50m, 0m),      # near left\n    WorldPoint(3000.0m, 50m, 0m),   # far left\n    WorldPoint(3000.0m, -50m, 0m),  # far right\n    WorldPoint(0.0m, -50m, 0m),     # near right\n]\n\ncam_pos = WorldPoint(-2000.0m, 12m, 150m)\ncam_rot = RotZYX(roll=1.5Â°, pitch=5Â°, yaw=0Â°)\n\ntrue_observations = [project(cam_pos, cam_rot, p) for p in runway_corners] |> SVector\nnoisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations] |> SVector\n\n@benchmark estimatepose6dof(PointFeatures(runway_corners, noisy_observations))","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"At the time of writing[1] the median benchmarking time for the pose estimate measures around 228.088 Î¼s (about 0.2 milliseconds) for one pose estimation. Quite fast! We also see that there is some spread, with typically up to 50% slower execution (0.3 milliseconds), with very rare extreme outliers taking way longer â€“ about 86 milliseconds at the time of writing. The cause of the rare outlier is still under investigation, and we assume interference of the CPU on the GitHub runner generating these results. Nonetheless, we conclude that we can estimate the pose from noisy point predictions extremely efficiently â€“ typically about five thousand times per second.","category":"page"},{"location":"benchmarks.html#Profiling-the-Callstack","page":"Benchmarks","title":"Profiling the Callstack","text":"","category":"section"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"To further highlight the efficiency of the presented code we can investigate the profiling trace of the call graph. Here we present a visualization thereof, although it is necessary to scroll down quite far to skip past the call stack of all the various \"entrypoints\".","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"using ProfileCanvas\n# once for precompile\n@profview map(1:1000) do _; estimatepose6dof(PointFeatures(runway_corners, noisy_observations)); end\n# now we measure\n@profview map(1:1000) do _; estimatepose6dof(PointFeatures(runway_corners, noisy_observations)); end","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"<br>","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"We can again see that we have gotten rid of almost all allocations (visualized in orange), except for pose_optimization_objective, which is currently a restriction of NonlinearSolve.jl. Nonetheless, we highlight that we spend almost the entire rest of the duration on actual compute rather than kernel calls such as allocations.","category":"page"},{"location":"benchmarks.html","page":"Benchmarks","title":"Benchmarks","text":"[1]: The outputs here are generated for every build.","category":"page"},{"location":"performance_tips.html#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"","category":"section"},{"location":"performance_tips.html","page":"Performance Tips","title":"Performance Tips","text":"TODO: Write this section.","category":"page"},{"location":"caches.html#Using-Caches","page":"Using Caches","title":"Using Caches","text":"","category":"section"},{"location":"caches.html","page":"Using Caches","title":"Using Caches","text":"TODO: Write this section","category":"page"},{"location":"python_interface.html#Python-Interface","page":"Python Interface","title":"Python Interface","text":"","category":"section"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"Several attempts have been made to make this module easily callable from python. At this time, we recommend using the python package JuliaCall. Simply install juliacall alongside numpy using your favorite method, for instance","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"uv add juliacall\nuv add numpy","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"and use juliacall to install RunwayLib and the PythonCall extension which enables the python API.","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"from juliacall import Main as jl\njl.Pkg.add(url=\"http://github.com/RomeoV/RunwayLib.jl\")  # will be registered soon","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"You only need to do this once. Now you should be able to use RunwayLib from python like so:","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"import numpy as np\nfrom juliacall import Main as jl\njl.seval(\"using RunwayLib, PythonCall\")\njl.seval(\"import RunwayLib: px, m\")\n\npoints2d = [\n    jl.ProjectionPoint(2284.8, 619.32)*jl.px,\n    jl.ProjectionPoint(2355.2, 620.90)*jl.px,\n    jl.ProjectionPoint(2399.5, 903.30)*jl.px,\n    jl.ProjectionPoint(2252.4, 904.09)*jl.px,\n]\npoints3d = [\n    jl.WorldPoint(1600, 15, -8)*jl.m,\n    jl.WorldPoint(1600, -15, -8)*jl.m,\n    jl.WorldPoint(0, -15, 0)*jl.m,\n    jl.WorldPoint(0, 15, 0)*jl.m,\n]\nintrinsic_matrix = np.array(\n    [\n        [-7311.8, 0.0, 2032.3],\n        [0.0, -7311.8, 1707.4],\n        [0.0, 0.0, 1.0],\n    ]\n)\n\ncamconf = jl.CameraMatrix[jl.Symbol(\"offset\")](\n    intrinsic_matrix, jl.px(2048.0), jl.px(1024.0),\n)\n\njl.seval(\"using RunwayLib.Unitful: ustrip\")\nres_jl = jl.estimatepose6dof(points3d, points2d, camconf)\npos = np.asarray(jl.broadcast(jl.ustrip, res_jl.pos))  # or `np.array(..., copy=None)`\nrot = np.asarray(res_jl.rot)\n\nfor i in range(1_000):\n    jl.estimatepose6dof(points3d, points2d, camconf)\n\n# Smoke test assertions\nassert pos.shape == (3,), f\"Expected position shape (3,), got {pos.shape}\"\nassert rot.shape == (3, 3), f\"Expected rotation shape (3, 3), got {rot.shape}\"\nprint(f\"âœ“ Smoke test passed! Position: {pos}, Rotation shape: {rot.shape}\")\n","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"Notice that we can also directly wrap np.array for WorldPoint and the other, e.g.:","category":"page"},{"location":"python_interface.html","page":"Python Interface","title":"Python Interface","text":"points2d_np = [\n    np.array([2284.8, 619.32]),\n    np.array([2355.2, 620.90]),\n    np.array([2399.5, 903.30]),\n    np.array([2252.4, 904.09]),\n]\npoint2d = [jl.ProjectionPoint(p) for p in points2d_np]","category":"page"},{"location":"test.html#Pose-Estimation-From-Lines","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"","category":"section"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"using WGLMakie: px as mpx\nusing WGLMakie","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"We start by setting up some basic runway corners and aircraft position.","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"using RunwayLib, Unitful.DefaultSymbols, Rotations\nusing RunwayLib: _ustrip\nusing Unitful: ustrip\nconst px = RunwayLib.px\n\nrunway_corners = [\n    WorldPoint(0.0m, 50m, 0m),     # near left\n    WorldPoint(3000.0m, 50m, 0m),  # far left\n    WorldPoint(3000.0m, -50m, 0m),  # far right\n    WorldPoint(0.0m, -50m, 0m),    # near right\n]\n\ncam_pos = WorldPoint(-2000.0m, 12m, 150m)\ncam_rot = RotZYX(roll=1.5Â°, pitch=5Â°, yaw=0Â°)\n\ntrue_observations = [project(cam_pos, cam_rot, p) for p in runway_corners]\nline_pts = [\n    (runway_corners[1], runway_corners[2]),\n    (runway_corners[3], runway_corners[4]),\n    ((runway_corners[1]+runway_corners[4])/2, (runway_corners[2]+runway_corners[3])/2),\n]\ntrue_lines = map(line_pts) do (p1, p2)\n    proj1 = project(cam_pos, cam_rot, p1)\n    proj2 = project(cam_pos, cam_rot, p2)\n    getline(proj1, proj2)\nend;","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"posests = map(1:500) do _\n    noisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations]\n    observed_lines = [\n      Line(r + 1px*randn(), theta + deg2rad(1Â°)*randn())\n      for (; r, theta) in true_lines\n    ]\n    (cam_pos_est, cam_rot_est) = estimatepose6dof(\n        PointFeatures(runway_corners[1:2], noisy_observations[1:2]),\n        LineFeatures(line_pts, observed_lines)\n    )[(:pos, :rot)]\n    cam_pos_est\nend;","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"with_theme(theme_black()) do\n    fig = Figure()#size=(1200, 800))\n    ax = LScene(fig[1,1]; show_axis=false)\n    Makie.scale!(ax.scene, 0.1,1,1)\n    scatter!(ax, [p .|> _ustrip(m) for p in posests])\n    fig\nend","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"Let's compare this with points estimated from all four corners, and from all four corners and the sideline angles.","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"posests_onlycorners = map(1:1500) do _\n    noisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations]\n    (cam_pos_est, cam_rot_est) = estimatepose6dof(\n        PointFeatures(runway_corners[1:4], noisy_observations[1:4]),\n    )[(:pos, :rot)]\n    cam_pos_est\nend;\nposests_allcornersandlines = map(1:1500) do _\n    noisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations]\n    observed_lines = [\n      Line(r + 1px*randn(), theta + deg2rad(1Â°)*randn())\n      for (; r, theta) in true_lines\n    ]\n    (cam_pos_est, cam_rot_est) = estimatepose6dof(\n        PointFeatures(runway_corners[1:4], noisy_observations[1:4]),\n        LineFeatures(line_pts, observed_lines)\n    )[(:pos, :rot)]\n    cam_pos_est\nend;\n\nwith_theme(theme_black()) do\n    fig = Figure()#size=(1000, 500))\n    sl = Makie.Slider(fig[3,1], range=0:0.01:1.0, startvalue=0.6)\n    ax = LScene(fig[1:2,1]; show_axis=false)\n    scale!(ax.scene, (0.1, 1, 1))\n    scatter!(ax, [p .|> _ustrip(m) for p in posests]; label=\"2 corners+lines\", alpha=sl.value)\n    scatter!(ax, [p .|> _ustrip(m) for p in posests_onlycorners]; label=\"4 corners\", alpha=sl.value)\n    scatter!(ax, [p .|> _ustrip(m) for p in posests_allcornersandlines]; label=\"4 corners+lines\", alpha=0.8)\n    fig[1,2] = Legend(fig, ax, \"Comparing Solutions\")\n    fig\nend","category":"page"},{"location":"test.html#What-if-we-know-some-more-information?","page":"Pose Estimation From Lines","title":"What if we know some more information?","text":"","category":"section"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"Let's say we can fix the altitude (e.g. because we know we're on the ground).","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"posests_fixedalt = map(1:500) do _\n    noisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations]\n    observed_lines = [\n      Line(r + 1px*randn(), theta + deg2rad(1Â°)*randn())\n      for (; r, theta) in true_lines\n    ]\n    (cam_pos_est, cam_rot_est) = estimatepose6dof(\n        PointFeatures(runway_corners[1:2], noisy_observations[1:2]),\n        LineFeatures(line_pts, observed_lines);\n        constraints=(3=>ustrip(m, cam_pos.z),)\n    )[(:pos, :rot)]\n    cam_pos_est\nend;\nwith_theme(theme_black()) do\n  fig = Figure()#size=(1000, 500))\n  sl = Makie.Slider(fig[3,1], range=0:0.01:1.0, startvalue=0.6)\n  ax = LScene(fig[1:2,1]; show_axis=false)\n  scale!(ax.scene, (0.1, 1, 1))\n  scatter!(ax, [p .|> _ustrip(m) for p in posests]; label=\"2 corners+lines\", alpha=sl.value)\n  scatter!(ax, [p .|> _ustrip(m) for p in posests_fixedalt]; label=\"+fixed altitude\", alpha=0.6)\n  fig[1,2] = Legend(fig, ax, \"Comparing Solutions\")\n  fig\nend","category":"page"},{"location":"test.html","page":"Pose Estimation From Lines","title":"Pose Estimation From Lines","text":"As we can see, fixing one of the variables works and is almost like \"slicing\" into the previous solution!","category":"page"},{"location":"camera_model.html#Camera-Model","page":"Camera Model","title":"Camera Model","text":"","category":"section"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"We use the pinhole camera model without any distortion. We implement two projection plane reference frames: :centered and :offset, which are defined as illustrated here:","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"using Typstry\nrender(typst\"\"\"\n       #include \"./figs/camera_models.typ\"\n       \"\"\"; output=\"camera_models.svg\", open=false)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"(Image: For offset, u point right and v points down. and the origin is in the top left. For centered, u points left and v points up, and the origin is at the image center.)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"We also support defining the camera model either through CameraConfig or through CameraMatrix.","category":"page"},{"location":"camera_model.html#:offset-vs-:centered","page":"Camera Model","title":":offset vs :centered","text":"","category":"section"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"using RunwayLib, Unitful.DefaultSymbols, Rotations\ncam_pos = WorldPoint(-10m, 0m, 0m)\ncam_rot = RotZYX(zeros(3)...)\nworld_pt = WorldPoint(0m, 0m, 0m)\n\nfocal_length = 25mm\npixel_size = 5Î¼m/px\ncamconf_centered = CameraConfig{:centered}(focal_length, pixel_size, 4096.0px, 2048.0px)\nproject(cam_pos, cam_rot, world_pt, camconf_centered)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"With an offset camera model:","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"camconf_offset = CameraConfig{:offset}(focal_length, pixel_size, 4096.0px, 2048.0px)\nproject(cam_pos, cam_rot, world_pt, camconf_offset)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"And for a non-centered point:","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"world_pt2 = WorldPoint(0m, 1m, 1m)\nproject(cam_pos, cam_rot, world_pt2, camconf_centered)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"project(cam_pos, cam_rot, world_pt2, camconf_offset)","category":"page"},{"location":"camera_model.html#Line-Projections","page":"Camera Model","title":"Line Projections","text":"","category":"section"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"For :offset camera models, we also currently support line features. Lines are specified with respect to a reference point chosen to be the \"offset origin\" and parameterized by their Hough transform, i.e., the angle and radius.","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"using Typstry\nrender(typst\"\"\"\n  #include \"./figs/line_parameterization.typ\"\n  \"\"\"; output=\"line_parameterization.svg\", open=false)","category":"page"},{"location":"camera_model.html","page":"Camera Model","title":"Camera Model","text":"(Image: )","category":"page"},{"location":"camera_model.html#Reference","page":"Camera Model","title":"Reference","text":"","category":"section"},{"location":"camera_model.html#RunwayLib.project-camera_model","page":"Camera Model","title":"RunwayLib.project","text":"function project(\n    cam_pos::WorldPoint{T}, cam_rot::RotZYX, world_pt::WorldPoint{Tâ€²},\n    camconfig::CameraConfig{S}=CAMERA_CONFIG_OFFSET\n) where {T,Tâ€²,S}\n\nProject 3D world point to 2D image coordinates using pinhole camera model. See Camera Model for more information.\n\n\n\n\n\nfunction project(\n    cam_pos::WorldPoint{T}, cam_rot::RotZYX, world_pt::WorldPoint{Tâ€²},\n    camconfig::CameraMatrix{S,U}\n) where {T,Tâ€²,S,U}\n\nVersion dispatching on CameraMatrix.\n\n\n\n\n\n","category":"function"},{"location":"camera_model.html#RunwayLib.CameraConfig-camera_model","page":"Camera Model","title":"RunwayLib.CameraConfig","text":"struct CameraConfig{S} <: RunwayLib.AbstractCameraConfig{S}\n\nCamera configuration with reference frame S being either :offset or :centered. Check Camera Model for further explanation.\n\nFields\n\nfocal_length_px::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\nimage_width::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\nimage_height::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\n\nExamples\n\nusing RunwayLib, Unitful.DefaultSymbols, Rotations\ncam_pos = WorldPoint(-10m, 0m, 0m)\ncam_rot = RotZYX(zeros(3)...)\nworld_pt = WorldPoint(0m, 0m, 0m)\n\nfocal_length = 25mm\npixel_size = 5Î¼m/px\ncamconf_centered = CameraConfig{:centered}(focal_length, pixel_size, 4096.0px, 2048.0px)\nproject(cam_pos, cam_rot, world_pt, camconf_centered); nothing\n\n\n\n\n\n","category":"type"},{"location":"camera_model.html#RunwayLib.CameraMatrix-camera_model","page":"Camera Model","title":"RunwayLib.CameraMatrix","text":"struct CameraMatrix{S,T<:WithDims(px)} <: AbstractCameraConfig{S}\n\nCamera model using 3x3 projection matrix with uniform pixel units. The reference frame S can either be :offset or :centered. See Camera Model for more explanation.\n\nnote: Note\nNotably it is the users responsibility to construct the matrix such that the axes are aligned correctly, i.e., for S=:offset the first two offdiagonal elements must be negative.\n\nwarning: Warning\nAt this time, only S = :offset is implemented.\n\nExamples\n\nusing StaticArrays, RunwayLib\nf_px = 5e6px  # focal length in pixels\ncx, cy = 2048px, 1024px\nmatrix = SA[\n    -f_px   0px  cx\n    0px   -f_px  cy\n    0px   0px   1px\n]\nCameraMatrix{:offset}(matrix, 2cx, 2cy); nothing\n\nRelated Functions\n\nSee also project.\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api_reference.html#RunwayLib.CAMCONF4COMP","page":"API Reference","title":"RunwayLib.CAMCONF4COMP","text":"Camera configuration type for precompilation\n\n\n\n\n\n","category":"constant"},{"location":"api_reference.html#RunwayLib.BehindCameraException","page":"API Reference","title":"RunwayLib.BehindCameraException","text":"BehindCameraException\n\nException thrown when the projection point is behind the camera\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.CameraConfig","page":"API Reference","title":"RunwayLib.CameraConfig","text":"struct CameraConfig{S} <: RunwayLib.AbstractCameraConfig{S}\n\nCamera configuration with reference frame S being either :offset or :centered. Check Camera Model for further explanation.\n\nFields\n\nfocal_length_px::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\nimage_width::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\nimage_height::Unitful.Quantity{Float64, NoDims, Unitful.FreeUnits{(pixel,), NoDims, nothing}}\n\nExamples\n\nusing RunwayLib, Unitful.DefaultSymbols, Rotations\ncam_pos = WorldPoint(-10m, 0m, 0m)\ncam_rot = RotZYX(zeros(3)...)\nworld_pt = WorldPoint(0m, 0m, 0m)\n\nfocal_length = 25mm\npixel_size = 5Î¼m/px\ncamconf_centered = CameraConfig{:centered}(focal_length, pixel_size, 4096.0px, 2048.0px)\nproject(cam_pos, cam_rot, world_pt, camconf_centered); nothing\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.CameraConfig-Union{Tuple{S}, Tuple{Unitful.Quantity{T, ð‹, U} where {T<:Real, U<:Unitful.Unitlike}, Unitful.Quantity{T, ð‹, U} where {T<:Real, U<:Unitful.Unitlike}, Unitful.Quantity{T, NoDims, U} where {T<:Real, U<:Unitful.Unitlike}, Unitful.Quantity{T, NoDims, U} where {T<:Real, U<:Unitful.Unitlike}}} where S","page":"API Reference","title":"RunwayLib.CameraConfig","text":"CameraConfig{S}(\n    focal_length::WithDims(mm), pixel_size::WithDims(mm / px),\n    image_width::WithDims(px), image_height::WithDims(px)\n)\n\nConvenience constructor for CameraConfig taking focal length and pixel size separately.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.CameraMatrix","page":"API Reference","title":"RunwayLib.CameraMatrix","text":"struct CameraMatrix{S,T<:WithDims(px)} <: AbstractCameraConfig{S}\n\nCamera model using 3x3 projection matrix with uniform pixel units. The reference frame S can either be :offset or :centered. See Camera Model for more explanation.\n\nnote: Note\nNotably it is the users responsibility to construct the matrix such that the axes are aligned correctly, i.e., for S=:offset the first two offdiagonal elements must be negative.\n\nwarning: Warning\nAt this time, only S = :offset is implemented.\n\nExamples\n\nusing StaticArrays, RunwayLib\nf_px = 5e6px  # focal length in pixels\ncx, cy = 2048px, 1024px\nmatrix = SA[\n    -f_px   0px  cx\n    0px   -f_px  cy\n    0px   0px   1px\n]\nCameraMatrix{:offset}(matrix, 2cx, 2cy); nothing\n\nRelated Functions\n\nSee also project.\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.CameraPoint","page":"API Reference","title":"RunwayLib.CameraPoint","text":"CameraPoint{T} <: FieldVector{3, T}\n\nPoint in camera coordinate system.\n\nFields\n\nx::T: Camera forward direction (positive towards scene)\ny::T: Camera right direction (positive to the right)\nz::T: Camera down direction (positive downward)\n\nUnits\n\nTypically uses meters (u\"m\") for all coordinates.\n\nCoordinate System Convention\n\nX-axis: Forward (into the scene)\nY-axis: Right (to the right of the camera)\nZ-axis: Down (downward from camera)\n\nThis follows the standard computer vision convention.\n\nExamples\n\n# Point 10m in front of camera, 2m to the right, 1m below\ncp = CameraPoint(10.0u\"m\", 2.0u\"m\", 1.0u\"m\")\n\n# Access coordinates\nprintln(\"Forward: \", cp.x)\nprintln(\"Right: \", cp.y)\nprintln(\"Down: \", cp.z)\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.LineFeatures","page":"API Reference","title":"RunwayLib.LineFeatures","text":"LineFeatures{T, Tâ€²â€², S, WL, OL, CC, M, Mâ€²}\n\nLine feature observations and noise model for pose estimation.\n\nFields\n\nworld_line_endpoints: Vector of pairs of WorldPoints defining lines in world space\nobserved_lines: Vector of Line objects (r, theta) from observations\ncamconfig: Camera configuration\ncov: Covariance matrix for observation errors\nLinv: Inverted lower triangular part of covariance\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.OptimizationConfig","page":"API Reference","title":"RunwayLib.OptimizationConfig","text":"OptimizationConfig\n\nConfiguration parameters for pose estimation optimization.\n\nFields\n\nmax_iterations::Int: Maximum number of optimization iterations\nconvergence_tolerance: Convergence tolerance for residual norm\nstep_tolerance: Minimum step size tolerance\ngradient_tolerance: Gradient norm tolerance for convergence\n\nExamples\n\nconfig = OptimizationConfig(\n    max_iterations = 100,\n    convergence_tolerance = 1e-6*1pixel,\n    step_tolerance = 1e-8,\n    gradient_tolerance = 1e-6\n)\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.PointFeatures","page":"API Reference","title":"RunwayLib.PointFeatures","text":"PointFeatures{T, Tâ€², Tâ€²â€², S, RC, OC, CC, M, Mâ€²}\n\nPoint feature observations and noise model for pose estimation.\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.PoseEstimate","page":"API Reference","title":"RunwayLib.PoseEstimate","text":"PoseEstimate\n\nComplete pose estimate with position, attitude, uncertainty, and convergence information.\n\nFields\n\nposition::WorldPoint: Estimated aircraft position in world coordinates\nattitude::RotZYX: Estimated aircraft attitude (yaw, pitch, roll)\nuncertainty::MvNormal: Joint position-attitude uncertainty distribution\nresidual_norm: Final residual norm from optimization (with units)\nconverged::Bool: Whether optimization converged successfully\n\nExamples\n\n# Create pose estimate\nposition = WorldPoint(500.0u\"m\", 10.0u\"m\", 100.0u\"m\")\nattitude = RotZYX(0.1, 0.05, 0.02)  # Small attitude angles\nuncertainty = MvNormal(zeros(6), I(6))  # 6-DOF uncertainty\nresidual_norm = 0.5*1pixel\nconverged = true\n\npose_est = PoseEstimate(position, attitude, uncertainty, residual_norm, converged)\n\n# Access components\nprintln(\"Position: \", pose_est.position)\nprintln(\"Attitude: \", pose_est.attitude)\nprintln(\"Converged: \", pose_est.converged)\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.PoseOptimizationParams3DOF","page":"API Reference","title":"RunwayLib.PoseOptimizationParams3DOF","text":"PoseOptimizationParams3DOF{A, PF, LF}\n\nParameters for 3-DOF pose optimization (position only with known attitude).\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.PoseOptimizationParams6DOF","page":"API Reference","title":"RunwayLib.PoseOptimizationParams6DOF","text":"PoseOptimizationParams6DOF{PF, LF}\n\nParameters for 6-DOF pose optimization (position + attitude).\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.ProjectionPoint","page":"API Reference","title":"RunwayLib.ProjectionPoint","text":"ProjectionPoint{T, S} <: FieldVector{2, T}\n\nPoint in image projection coordinate system.\n\nType Parameters\n\nT: Numeric type for coordinates\nS: Coordinate system type (only :offset supported)\n\nFields\n\nx::T: Image x-coordinate (horizontal pixel position)\ny::T: Image y-coordinate (vertical pixel position)\n\nUnits\n\nTypically uses pixels (1pixel) for coordinates.\n\nCoordinate System Convention\n\nFor :offset coordinates:\n\nOrigin at top-left corner of image\nX-axis: Horizontal (positive to the right)\nY-axis: Vertical (positive downward)\n\nExamples\n\n# Offset coordinates (origin at top-left)\npp_offset = ProjectionPoint{Float64, :offset}(1024.0*1pixel, 768.0*1pixel)\n\n# Access coordinates\nprintln(\"X: \", pp_offset.x)\nprintln(\"Y: \", pp_offset.y)\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#RunwayLib.WorldPoint","page":"API Reference","title":"RunwayLib.WorldPoint","text":"WorldPoint{T} <: FieldVector{3, T}\n\nPoint in world coordinate system (runway-relative).\n\nFields\n\nx::T: Along-track distance (positive towards far end of runway)\ny::T: Cross-track distance (positive towards right side of runway)\nz::T: Height above runway surface (positive upward)\n\nUnits\n\nTypically uses meters (u\"m\") for all coordinates.\n\nExamples\n\n# Create a point 500m before runway threshold, 10m right of centerline, 100m high\nwp = WorldPoint(-500.0u\"m\", 10.0u\"m\", 100.0u\"m\")\n\n# Access coordinates\nprintln(\"Along-track: \", wp.x)\nprintln(\"Cross-track: \", wp.y) \nprintln(\"Height: \", wp.z)\n\n# Arithmetic operations\nwp2 = WorldPoint(100.0u\"m\", 0.0u\"m\", 50.0u\"m\")\nwp_sum = wp + wp2  # Element-wise addition\nwp_scaled = 2.0 * wp  # Scalar multiplication\n\n\n\n\n\n","category":"type"},{"location":"api_reference.html#Base.inv-Union{Tuple{LinearAlgebra.LowerTriangular{T, <:StaticArraysCore.SMatrix{N, N}}}, Tuple{N}, Tuple{T}} where {T, N}","page":"API Reference","title":"Base.inv","text":"inv(L::LowerTriangular{T, <:SMatrix}) where T\n\nCustom inverse for lower triangular static matrices using forward-substitution. Preserves SMatrix type instead of converting to Matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#Base.inv-Union{Tuple{LinearAlgebra.UpperTriangular{T, <:StaticArraysCore.SMatrix{N, N}}}, Tuple{N}, Tuple{T}} where {T, N}","page":"API Reference","title":"Base.inv","text":"inv(U::UpperTriangular{T, <:SMatrix}) where T\n\nCustom inverse for upper triangular static matrices using back-substitution. Preserves SMatrix type instead of converting to Matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#NonlinearSolveBase.fix_incompatible_linsolve_arguments-Union{Tuple{T}, Tuple{LinearAlgebra.Symmetric{T, <:Matrix{T}}, Any, StaticArraysCore.SArray}} where T","page":"API Reference","title":"NonlinearSolveBase.fix_incompatible_linsolve_arguments","text":"See #107\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.WithDims-Tuple{Unitful.Quantity}","page":"API Reference","title":"RunwayLib.WithDims","text":"WithDims(q::Quantity)\nWithDims(u::Units)\n\nReturns a subtype of Unitful.Quantity with the dimensions constrained to the dimension of q or u. Useful to build unitful interfaces that don't constrain the numeric type or the unit, just the dimension of a quantity. Examples:\n\njulia> using Unitful, Unitful.DefaultSymbols; import Unitful.hr\n\njulia> using RunwayLib: WithDims\n\njulia> circumference_of_square(side::WithDims(m)) = 4*side;\n\njulia> circumference_of_square((1//2)m)  # works\n2//1 m\n\njulia> circumference_of_square((1//2)km)  # also works\n2//1 km\n\njulia> # You can also constrain the return type. The numeric type is usually inferred automatically.\n\njulia> kinetic_energy(mass::WithDims(kg), velocity::WithDims(m/s))::WithDims(J) = mass*velocity^2;\n\njulia> kinetic_energy(1000kg, 100km/hr)\n10000000 kg km^2 hr^-2\n\nSee also WithUnits.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.WithUnits-Tuple{Unitful.Quantity}","page":"API Reference","title":"RunwayLib.WithUnits","text":"WithUnits(q::Quantity)\nWithUnits(u::Units)\n\nReturns a subtype of Unitful.Quantity with the dimensions and units constrained to the dimension and units of q or u. Useful to build unitful interfaces that don't constrain the unit, but not the numeric type of a quantity. Examples:\n\njulia> using Unitful, Unitful.DefaultSymbols; import Unitful.hr\n\njulia> using RunwayLib: WithUnits\n\njulia> circumference_of_square(side::WithUnits(m)) = 4*side;\n\njulia> circumference_of_square((1//2)m)  # works\n2//1 m\n\njulia> # circumference_of_square((1//2)km)  # doesn't work, constrained to exactly meters\n\njulia> # You can also constrain the return type. The numeric type is usually inferred automatically.\n\njulia> kinetic_energy(mass::WithUnits(kg), velocity::WithUnits(m/s))::WithUnits(J) = mass*velocity^2 |> x->uconvert(J, x);\n\njulia> kinetic_energy(1000kg, uconvert(m/s, 100km/hr))\n62500000//81 J\n\nSee also WithDims.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.cam_pt_to_world_pt-Tuple{WorldPoint, Rotations.RotZYX, CameraPoint}","page":"API Reference","title":"RunwayLib.cam_pt_to_world_pt","text":"cam_pt_to_world_pt(cam_pos::WorldPoint, R::RotZYX, cam_pt::CameraPoint) -> WorldPoint\n\nTransform a point from camera coordinates to world coordinates.\n\nArguments\n\ncam_pos::WorldPoint: Camera position in world coordinates\ncam_rot::RotZYX: Camera orientation (ZYX Euler angles: yaw, pitch, roll)\ncam_pt::CameraPoint: Point to transform in camera coordinates\n\nReturns\n\nWorldPoint: Point in world coordinate system\n\nAlgorithm\n\nRotate point by camera rotation to get world-relative coordinates\nTranslate by camera position to get absolute world coordinates\n\nExamples\n\nusing RunwayLib, Unitful.DefaultSymbols, Rotations\n# Transform camera point back to world coordinates\ncam_pos = WorldPoint(10.0m, 20m, 30m)\ncam_rot = RotZYX(roll=0.0rad, pitch=0rad, yaw=0rad)  # No rotation\ncam_pt = CameraPoint(1.0m, 2m, 3m)\n\nworld_pt = cam_pt_to_world_pt(cam_pos, cam_rot, cam_pt)\n\n# output\n\n3-element WorldPoint{Float64{m}} with indices SOneTo(3):\n 11.0 m\n 22.0 m\n 33.0 m\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.compute_integrity_statistic","page":"API Reference","title":"RunwayLib.compute_integrity_statistic","text":"compute_integrity_statistic(\n    cam_pos::WorldPoint, cam_rot::RotZYX,\n    world_pts::AbstractVector{<:WorldPoint},\n    observed_pts::AbstractVector{<:ProjectionPoint},\n    noise_cov::Union{<:AbstractMatrix, <:NoiseModel},\n    camconfig=CAMERA_CONFIG_OFFSET\n)\n\nRun the integrity check described in [1]. We can use this for runtime assurance to judge whether the measurements and uncertainties are consistent with the parameters of the problem.\n\nReturns\n\nNamedTuple containing\n\nstat: The RAIM-adaptation statistic;\np_value: p-value of Null-hypothesis. If this drops below, say, 5% then we can \"reject\", i.e., have a failure;\ndofs: degrees of freedom (for Î§Â² distribution); and\nsome other information.\n\nSee also\n\nWorldPoint, RotZYX, ProjectionPoint, NoiseModel.\n\n\n\n\n\n","category":"function"},{"location":"api_reference.html#RunwayLib.getline-Union{Tuple{Tâ€²}, Tuple{T}, Tuple{ProjectionPoint{T, :offset}, ProjectionPoint{Tâ€², :offset}}} where {T, Tâ€²}","page":"API Reference","title":"RunwayLib.getline","text":"getline(p1::ProjectionPoint, p2::ProjectionPoint) -> Line\n\nConvert two projection points to Hough transform parameters (r, theta). Line represented as: r = xcos(theta) + ysin(theta)\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.makecache-Tuple{Any, RunwayLib.AbstractPoseOptimizationParams}","page":"API Reference","title":"RunwayLib.makecache","text":"makecache(uâ‚€, ps::AbstractPoseOptimizationParams)\n\nCreate optimization cache.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.nominal2optvar-Union{Tuple{AT}, Tuple{AT, PoseOptimizationParams3DOF}} where AT","page":"API Reference","title":"RunwayLib.nominal2optvar","text":"From regular space into optimization space.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.optvar2nominal-Union{Tuple{AT}, Tuple{AT, PoseOptimizationParams3DOF}} where AT","page":"API Reference","title":"RunwayLib.optvar2nominal","text":"From optimization space into regular space.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.pose_optimization_objective-Union{Tuple{T}, Tuple{AbstractVector{T}, RunwayLib.AbstractPoseOptimizationParams}} where T<:Real","page":"API Reference","title":"RunwayLib.pose_optimization_objective","text":"pose_optimization_objective(pose_params, ps)\n\nUnified optimization function for pose estimation.\n\nArguments\n\npose_params: Vector of pose parameters\n[x, y, z, roll, pitch, yaw] for 6-DOF\n[x, y, z] for 3-DOF\nps: PoseOptimizationParams6DOF or PoseOptimizationParams3DOF\n\nReturns\n\nWeighted reprojection error vector combining point and line features\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.pose_optimization_objective_lines-Tuple{WorldPoint, Rotations.Rotation, LineFeatures}","page":"API Reference","title":"RunwayLib.pose_optimization_objective_lines","text":"pose_optimization_objective_lines(cam_pos, cam_rot, line_features)\n\nCompute weighted line feature residuals.\n\nArguments\n\ncam_pos: Camera position (WorldPoint)\ncam_rot: Camera rotation (Rotation)\nline_features: LineFeatures struct\n\nReturns\n\nWeighted line error vector (empty if no lines)\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.pose_optimization_objective_points-Tuple{WorldPoint, Rotations.Rotation, PointFeatures}","page":"API Reference","title":"RunwayLib.pose_optimization_objective_points","text":"pose_optimization_objective_points(cam_pos, cam_rot, point_features)\n\nCompute weighted point feature residuals.\n\nArguments\n\ncam_pos: Camera position (WorldPoint)\ncam_rot: Camera rotation (Rotation)\npoint_features: PointFeatures struct\n\nReturns\n\nWeighted reprojection error vector\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.project-Union{Tuple{S}, Tuple{Tâ€²}, Tuple{T}, Tuple{WorldPoint{T}, Rotations.RotZYX, WorldPoint{Tâ€²}}, Tuple{WorldPoint{T}, Rotations.RotZYX, WorldPoint{Tâ€²}, CameraConfig{S}}} where {T, Tâ€², S}","page":"API Reference","title":"RunwayLib.project","text":"function project(\n    cam_pos::WorldPoint{T}, cam_rot::RotZYX, world_pt::WorldPoint{Tâ€²},\n    camconfig::CameraConfig{S}=CAMERA_CONFIG_OFFSET\n) where {T,Tâ€²,S}\n\nProject 3D world point to 2D image coordinates using pinhole camera model. See Camera Model for more information.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.project-Union{Tuple{U}, Tuple{S}, Tuple{Tâ€²}, Tuple{T}, Tuple{WorldPoint{T}, Rotations.RotZYX, WorldPoint{Tâ€²}, CameraMatrix{S, U}}} where {T, Tâ€², S, U}","page":"API Reference","title":"RunwayLib.project","text":"function project(\n    cam_pos::WorldPoint{T}, cam_rot::RotZYX, world_pt::WorldPoint{Tâ€²},\n    camconfig::CameraMatrix{S,U}\n) where {T,Tâ€²,S,U}\n\nVersion dispatching on CameraMatrix.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.validate_camera_matrix-Tuple{StaticArraysCore.SMatrix{3, 3}, Any}","page":"API Reference","title":"RunwayLib.validate_camera_matrix","text":"Validate 3x3 matrix for camera projection.\n\n\n\n\n\n","category":"method"},{"location":"api_reference.html#RunwayLib.world_pt_to_cam_pt-Union{Tuple{Tâ€²}, Tuple{T}, Tuple{WorldPoint{T}, Rotations.RotZYX, WorldPoint{Tâ€²}}} where {T<:(Union{Unitful.Quantity{T, ð‹, U}, Unitful.Level{L, S, Unitful.Quantity{T, ð‹, U}} where {L, S}} where {T, U}), Tâ€²<:(Union{Unitful.Quantity{T, ð‹, U}, Unitful.Level{L, S, Unitful.Quantity{T, ð‹, U}} where {L, S}} where {T, U})}","page":"API Reference","title":"RunwayLib.world_pt_to_cam_pt","text":"world_pt_to_cam_pt(cam_pos::WorldPoint, R::RotZYX, world_pt::WorldPoint) -> CameraPoint\n\nTransform a point from world coordinates to camera coordinates.\n\nArguments\n\ncam_pos::WorldPoint: Camera position in world coordinates\ncam_rot::RotZYX: Camera orientation (ZYX Euler angles: yaw, pitch, roll)\nworld_pt::WorldPoint: Point to transform in world coordinates\n\nReturns\n\nCameraPoint: Point in camera coordinate system\n\nAlgorithm\n\nTranslate point relative to camera position\nRotate by inverse of camera rotation to get camera-relative coordinates\n\nExamples\n\nusing RunwayLib, Unitful.DefaultSymbols, Rotations\n# Camera at origin with no rotation\ncam_pos = WorldPoint(0.0m, 0m, 0m)\ncam_rot = RotZYX(roll=0.0rad, pitch=0rad, yaw=0rad)  # No rotation\nworld_pt = WorldPoint(1.0m, 2m, 3m)\n\ncam_pt = world_pt_to_cam_pt(cam_pos, cam_rot, world_pt)\n\n# output\n\n3-element WorldPoint{Float64{m}} with indices SOneTo(3):\n 1.0 m\n 2.0 m\n 3.0 m\n\nWith some rotation:\n\n# Camera with 90-degree yaw rotation\ncam_rot = RotZYX(yaw=Ï€/2rad, roll=0.0rad, pitch=0.0rad)  # 90-degree yaw\nworld_pt = WorldPoint(1.0m, 0.0m, 0.0m)\n\ncam_pt = world_pt_to_cam_pt(cam_pos, cam_rot, world_pt)\n# After rotation, x becomes -y in camera frame\ncam_pt â‰ˆ WorldPoint(0.0m, -1.0m, 0.0m)\n\n# output\n\ntrue\n\n\n\n\n\n","category":"method"},{"location":"index.html#RunwayLib.jl","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl","text":"","category":"section"},{"location":"index.html#Getting-Started","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"Getting Started","text":"","category":"section"},{"location":"index.html","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","text":"using RunwayLib, Unitful.DefaultSymbols, Rotations\n\nrunway_corners = [\n    WorldPoint(0.0m, 50m, 0m),     # near left\n    WorldPoint(3000.0m, 50m, 0m),  # far left\n    WorldPoint(3000.0m, -50m, 0m),  # far right\n    WorldPoint(0.0m, -50m, 0m),    # near right\n]\n\ncam_pos = WorldPoint(-2000.0m, 12m, 150m)\ncam_rot = RotZYX(roll=1.5Â°, pitch=5Â°, yaw=0Â°)\n\ntrue_observations = [project(cam_pos, cam_rot, p) for p in runway_corners]\nnoisy_observations = [p + ProjectionPoint(2.0*randn(2)px) for p in true_observations]\n\n(cam_pos_est, cam_rot_est) = estimatepose6dof(\n    PointFeatures(runway_corners, noisy_observations)\n)[(:pos, :rot)]\n\ncam_pos_est","category":"page"},{"location":"index.html","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","text":"We can extract roll-pitch-yaw as","category":"page"},{"location":"index.html","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","text":"import Rotations: params\n(yaw, pitch, roll) = params(cam_rot_est)\n@show rad2deg(roll*rad)\n@show rad2deg(pitch*rad)\n@show rad2deg(yaw*rad)\n;","category":"page"},{"location":"index.html#Using-Line-Features","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"Using Line Features","text":"","category":"section"},{"location":"index.html","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","text":"Besides point features we can additionally include line features which can typically improve our altitude and crosstrack estimations, but usually can't improve our alongtrack estimation much because the line projections are constant along the glidepath. See Line Projections for more information on the line parameterization.","category":"page"},{"location":"index.html","page":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","title":"RunwayLib.jl: Fast Pose Estimation and Runtime Assurance for Runway Landings.","text":"line_pts = [\n    (runway_corners[1], runway_corners[2]),\n    (runway_corners[3], runway_corners[4]),\n]\ntrue_lines = map(line_pts) do (p1, p2)\n    proj1 = project(cam_pos, cam_rot, p1)\n    proj2 = project(cam_pos, cam_rot, p2)\n    getline(proj1, proj2)\nend\nobserved_lines = [\n  Line(\n    r + 1px*randn(),\n    theta + deg2rad(1Â°)*randn()\n  )\n  for (; r, theta) in true_lines\n]\n\n# now with additional line features\n(cam_pos_est, cam_rot_est) = estimatepose6dof(\n    PointFeatures(runway_corners, noisy_observations),\n    LineFeatures(line_pts, observed_lines)\n)[(:pos, :rot)]\n\ncam_pos_est","category":"page"}]
}
